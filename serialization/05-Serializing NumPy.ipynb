{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Erudio logo](../img/erudio-logo-small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Saving and Loading NumPy Arrays\n",
    "\n",
    "While not part of Python itself, the NumPy array library forms the basis for nearly all numeric computation within Python.  A few core features of the Python language have been specialized to accomodate the NumPy community and library.  The most notable examples of the language definition being modified for the sake of NumPy are the extended slice notation and the matrix multiply operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *stride* argument to slices was added in Python 1.4 (long ago), but was not used by Python lists, tuples, or strings until 2.3 (still a long time).  The use of commas within compound slice descriptions is not used anywhere in Python's standard library but exists so that NumPy (and later other libraries) can utilize it.  Similarly, the operator `@` is not used anywhere in Python itself or its standard library, but was added so that NumPy can use it to denote matrix multiplication (some other libraries have utilized it for other purposes since then)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm tmp/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Serializing with Pickle\n",
    "\n",
    "Most Python objects, even those in extension libraries, can be serialized and deserialized with `pickle` module.  Classes are able to define a few protocol methods that allow them to interoperate with pickling.  For most purposes, pickle is fine for representing NumPy arrays.  Let us create one and roundtrip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.random.randint(1, 100, 1_000_000).reshape(100, 100, 100)\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1680,  308, 1120, 1204],\n",
       "       [4380,  803, 2920, 3139],\n",
       "       [3000,  550, 2000, 2150],\n",
       "       [1680,  308, 1120, 1204]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Syntax extras for NumPy (matrix multiply dimensional slices)\n",
    "arr[2:6, 4, 3:4] @ arr[8:9, 3, 4:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dumping and loading a pickle of a NumPy array is the same as for any Python object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[24, 37, 59, ..., 89,  8,  7],\n",
       "        [67, 30, 24, ...,  4, 47, 21],\n",
       "        [32, 22, 29, ..., 41, 35, 98],\n",
       "        ...,\n",
       "        [12, 88, 38, ..., 42, 60, 69],\n",
       "        [97, 70, 16, ..., 72, 50, 39],\n",
       "        [88, 89, 98, ..., 33, 45, 49]],\n",
       "\n",
       "       [[11, 47, 24, ..., 82, 28,  6],\n",
       "        [91, 12,  4, ..., 15, 90, 89],\n",
       "        [34, 67, 84, ..., 76, 35, 59],\n",
       "        ...,\n",
       "        [69, 72, 92, ..., 90, 62, 56],\n",
       "        [ 6, 67, 55, ..., 82, 46, 86],\n",
       "        [87, 33, 35, ..., 32,  3, 50]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.dump(arr, open('tmp/arr.pkl', 'wb'))\n",
    "arr2 = pickle.load(open('tmp/arr.pkl', 'rb'))\n",
    "arr2[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An advantage of pickles is that you might embed an array inside other structures, and pickle will handle that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x80\\x04\\x95\\x90\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\x05array\\x94\\x8c\\x15numpy.core.'\n",
      "Description: A million random integers\n",
      "Extra data: [0. 1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "data = {'array': arr,\n",
    "        'description': \"A million random integers\",\n",
    "        'list': [5.4, 9.1, 3.4],\n",
    "        'another_array': np.arange(5.0)}\n",
    "\n",
    "data_bytes = pickle.dumps(data)\n",
    "print(data_bytes[:35])\n",
    "\n",
    "new_data = pickle.loads(data_bytes)\n",
    "print(\"Description:\", new_data['description'])\n",
    "print(\"Extra data:\", new_data['another_array'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Serializing with `np.savetxt()`\n",
    "\n",
    "For NumPy arrays that are 1-D or 2-D, you can save them as delimited files with the `savetxt()` function.  This is a convenient way to save data to CSV or TSV that might be read by DataFrame libraries or similar tools.  However, multi-dimensional arrays need to be reduced to 2-D to be stored in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt('tmp/arr-txt.tsv', \n",
    "           arr.reshape(100_000, 10),\n",
    "           delimiter='\\t',\n",
    "           fmt='%d',\n",
    "           header='Original shape: (100, 100, 100)',\n",
    "           comments='# ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Original shape: (100, 100, 100)\n",
      "24\t37\t59\t87\t45\t14\t2\t80\t90\t96\n",
      "35\t68\t62\t24\t66\t22\t38\t59\t96\t7\n",
      "58\t13\t61\t38\t89\t93\t70\t76\t1\t25\n",
      "98\t98\t99\t70\t72\t50\t67\t3\t83\t14\n",
      "34\t53\t65\t48\t84\t54\t72\t39\t92\t16\n",
      "31\t76\t34\t77\t55\t74\t48\t6\t28\t15\n",
      "99\t77\t41\t74\t36\t84\t55\t94\t34\t92\n",
      "21\t28\t14\t4\t82\t72\t20\t61\t7\t13\n",
      "48\t49\t30\t53\t96\t10\t39\t25\t94\t58\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head tmp/arr-txt.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Serializing with `np.save()`\n",
    "\n",
    "The native NumPy serialization format is very simple and directly represents arrays on disk.  An `.npy` file is *slightly* faster to write than a pickle, and is *slightly* smaller on disk.  These differences are minimal and are swamped by disk caching effects and data size, respectively.  The actual advantage of `.npy` is precisely what it *does not* do; reading a serialized array will never instantiate custom classes, will never execute arbitrary code, and will never contain structures other than arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 3.51 ms\n"
     ]
    }
   ],
   "source": [
    "%time np.save('tmp/arr.npy', arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[24, 37, 59, ..., 89,  8,  7],\n",
       "        [67, 30, 24, ...,  4, 47, 21],\n",
       "        [32, 22, 29, ..., 41, 35, 98],\n",
       "        ...,\n",
       "        [12, 88, 38, ..., 42, 60, 69],\n",
       "        [97, 70, 16, ..., 72, 50, 39],\n",
       "        [88, 89, 98, ..., 33, 45, 49]],\n",
       "\n",
       "       [[11, 47, 24, ..., 82, 28,  6],\n",
       "        [91, 12,  4, ..., 15, 90, 89],\n",
       "        [34, 67, 84, ..., 76, 35, 59],\n",
       "        ...,\n",
       "        [69, 72, 92, ..., 90, 62, 56],\n",
       "        [ 6, 67, 55, ..., 82, 46, 86],\n",
       "        [87, 33, 35, ..., 32,  3, 50]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr3 = np.load('tmp/arr.npy')\n",
    "arr3[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Serializing with `np.savez` and `np.savez_compressed`\n",
    "\n",
    "An enhancement to the `.npy` format is the `.npz` format.  This uses a zipfile wrapper to aggregate multiple arrays in the same file.  Again, pickle could do this by putting them inside a dict or a list; the restriction is exactly the advantage for some cases.  In general, the compressed version is to be preferred in almost all cases; for the last decade, the extra CPU cycles to perform compression have been almost always faster than the extra time required to write more data to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.savez('tmp/arr', arr, data['another_array'])\n",
    "np.savez_compressed('tmp/arr-compress', arr, data['another_array'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The multiple arrays are available in a dict-like interface, and simply named as `arr_0`, `arr_1`, and so on.  You must store any mapping to the variable names used for these by separate means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_0 (100, 100, 100) int32\n",
      "arr_1 (5,) float64\n",
      "[0. 1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "arr_data = np.load('tmp/arr.npz')\n",
    "for name in arr_data:\n",
    "    print(name, arr_data[name].shape, arr_data[name].dtype)\n",
    "\n",
    "print(arr_data['arr_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# File sizes\n",
    "\n",
    "We serialized the same data using several different formats.  The CPU times taken for all of them are neglibigle; there are some notable patterns in disk usage.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 15912\n",
      "drwxrwxrwx 1 laishaw laishaw     512 Feb 13 07:39 .\n",
      "drwxrwxrwx 1 laishaw laishaw     512 Feb 13 07:38 ..\n",
      "-rwxrwxrwx 1 laishaw laishaw 1277898 Feb 13 07:39 arr-compress.npz\n",
      "-rwxrwxrwx 1 laishaw laishaw 3009490 Feb 13 07:39 arr-txt.tsv\n",
      "-rwxrwxrwx 1 laishaw laishaw 4000128 Feb 13 07:39 arr.npy\n",
      "-rwxrwxrwx 1 laishaw laishaw 4000546 Feb 13 07:39 arr.npz\n",
      "-rwxrwxrwx 1 laishaw laishaw 4000163 Feb 13 07:39 arr.pkl\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -la tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice the initially surprising fact that the text format is not the largest.  Because all of our integers were only two digits, they were each stored with two bytes for the numbers plus one for the delimiter.  In contrast, an int64 value requires 8 bytes to store uncompressed.  If the data contained values closer to `sys.maxsize`, i.e. 9,223,372,036,854,775,807, the size of the text version could easily become larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "Materials licensed under [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/) by the authors"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
